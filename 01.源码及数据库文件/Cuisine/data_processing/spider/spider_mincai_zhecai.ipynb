{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 浙菜、闽菜相关信息爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需要的包\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成请求头\n",
    "request_headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.82\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Referer\": \"https://home.meishichina.com/recipe-type.html\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 浙菜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END...\n"
     ]
    }
   ],
   "source": [
    "# 生成 urls list\n",
    "urls_zhecai = ['https://home.meishichina.com/recipe/zhecai/page/{}/'.format(str(i)) for i in range(1, 4)]\n",
    "data_lst = []\n",
    "# 遍历每一个列表页\n",
    "for url_zhecai in urls_zhecai:\n",
    "    # 封装发送请求并获取响应\n",
    "    response = requests.get(url=url_zhecai, headers=request_headers)\n",
    "    # 判断状态码\n",
    "    status_code = response.status_code\n",
    "    if status_code == 200:\n",
    "        html_text = response.text\n",
    "        # 解析 html 文本\n",
    "        bs = BeautifulSoup(html_text, \"lxml\")\n",
    "        # 定位到详情页链接的 a 标签\n",
    "        urls_cuisine = bs.select('#J_list > ul > li > div.detail > h2 > a')\n",
    "        img_infos = bs.select('#J_list > ul > li > div.pic > a > img')\n",
    "        # 遍历所有菜品\n",
    "        for cuisine, img_info in zip(urls_cuisine, img_infos):\n",
    "            # 提取详情页链接，即 href 属性值\n",
    "            url_cuisine = cuisine.get('href')\n",
    "            # 由于尝试发现详情页中图片不合适，所以回到列表页抓取图片\n",
    "            cuisine_name = cuisine.get('title')\n",
    "            pic_url = img_info.get('data-src')\n",
    "            resp = requests.get(pic_url, headers=request_headers)\n",
    "            # 检查响应状态码\n",
    "            if resp.status_code == 200:\n",
    "                # 获取图片内容\n",
    "                image_content = resp.content\n",
    "                file_name = cuisine_name + \".jpg\"\n",
    "                file_path = os.path.join(r'D:\\PYZ\\PT(Shi_xi_shi_xun)\\Group\\menu_tasks\\pictures', 'zhe')\n",
    "                if not os.path.exists(file_path):\n",
    "                    os.mkdir(file_path)\n",
    "\n",
    "                # 完成写入\n",
    "                with open(file_path + os.sep + file_name, 'wb') as fp:\n",
    "                    fp.write(image_content)\n",
    "\n",
    "            # 获取每一个菜品详情页面中的信息\n",
    "            # 封装发送请求并获取响应\n",
    "            response = requests.get(url=url_cuisine, headers=request_headers)\n",
    "            # 判断状态码\n",
    "            status_code = response.status_code\n",
    "            if status_code == 200:\n",
    "                # 防止中文乱码，因此自动识别编码方式后再提取文本\n",
    "                response.encoding = response.apparent_encoding\n",
    "                html_text = response.text\n",
    "                # 解析 html 文本\n",
    "                bs = BeautifulSoup(html_text, \"lxml\")\n",
    "                selector = etree.HTML(html_text)\n",
    "                # 获取菜品名称\n",
    "                cuisine_name = bs.select('body > div.wrap > div > div.space_left > div.userTop.clear > h1 > a')[0].get_text().strip()\n",
    "                # print(cuisine_name)\n",
    "                # 获取菜品主料\n",
    "                # 悲，本来觉得不难！结果从这第二个数据就卡死了难蚌！太难了！！！最难得是定位啊！！！有规律的网页是正规网页人家会反爬，哭；不反爬的小网页，它可能不规律，淦！\n",
    "                # 由于使用CSS选择器的定位方式，发现定位不好搞，因为fieldset后面的数字不同，也就是层数不一样（悲），但是！经过桐桐的提醒，发现xpath是有规律的！所以站在巨人的肩膀上干活效率会变高（bushi）\n",
    "                main_ingredient_dict = {}\n",
    "                main_ingredient_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[1]/div/ul/li/span[1]/a/b')\n",
    "                for i in range(0,len(main_ingredient_names)):\n",
    "                    main_ingredient_name = main_ingredient_names[i].text\n",
    "                    main_ingredient_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[1]/div/ul/li/span[2]')\n",
    "                    main_ingredient_dict[main_ingredient_name] = main_ingredient_dosage[i].text\n",
    "                # 这里是处理没有链接的那种情况，两种定位不一样\n",
    "                main_ingredient_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[1]/div/ul/li/span[1]/b')\n",
    "                if len(main_ingredient_names) != 0:\n",
    "                    for i in range(0,len(main_ingredient_names)):\n",
    "                        main_ingredient_name = main_ingredient_names[i].text\n",
    "                        main_ingredient_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[1]/div/ul/li/span[2]')\n",
    "                        main_ingredient_dict[main_ingredient_name] = main_ingredient_dosage[i].text\n",
    "                # print(main_ingredient_dict)\n",
    "\n",
    "                # 获取菜品辅料\n",
    "                # 为了避免麻烦，继续采用上面这个方法\n",
    "                secondary_ingredient_dict = {}\n",
    "                secondary_ingredient_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[2]/div/ul/li/span[1]/a/b')\n",
    "                for i in range(0,len(secondary_ingredient_names)):\n",
    "                    secondary_ingredient_name = secondary_ingredient_names[i].text\n",
    "                    secondary_ingredient_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[2]/div/ul/li/span[2]')\n",
    "                    secondary_ingredient_dict[secondary_ingredient_name] = secondary_ingredient_dosage[i].text\n",
    "                # 这里是处理没有链接的那种情况，两种定位不一样\n",
    "                secondary_ingredient_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[2]/div/ul/li/span[1]/b')\n",
    "                if len(secondary_ingredient_names) != 0:\n",
    "                    for i in range(0,len(secondary_ingredient_names)):\n",
    "                        secondary_ingredient_name = secondary_ingredient_names[i].text\n",
    "                        secondary_ingredient_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[2]/div/ul/li/span[2]')\n",
    "                        secondary_ingredient_dict[secondary_ingredient_name] = secondary_ingredient_dosage[i].text\n",
    "                # print(secondary_ingredient_dict)\n",
    "\n",
    "                # 获取菜品调料\n",
    "                # 突出一个追求效率（潜台词：懒）\n",
    "                spices_dict = {}\n",
    "                spices_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[3]/div/ul/li/span[1]/a/b')\n",
    "                for i in range(0,len(spices_names)):\n",
    "                    spices_name = spices_names[i].text\n",
    "                    spices_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[3]/div/ul/li/span[2]')\n",
    "                    spices_dict[spices_name] = spices_dosage[i].text\n",
    "                # 这里是处理没有链接的那种情况，两种定位不一样\n",
    "                spices_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[3]/div/ul/li/span[1]/b')\n",
    "                if len(spices_names) != 0:\n",
    "                    for i in range(0,len(spices_names)):\n",
    "                        spices_name = spices_names[i].text\n",
    "                        spices_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[3]/div/ul/li/span[2]')\n",
    "                        spices_dict[spices_name] = spices_dosage[i].text\n",
    "                # print(spices_dict)\n",
    "\n",
    "                # 获取食材明细最后一块 —— 其实应该是特性\n",
    "                # 口味 工艺 耗时 难度\n",
    "                detail_info = {\n",
    "                    '口味': None,\n",
    "                    '工艺': None,\n",
    "                    '耗时': None,\n",
    "                    '难度': None\n",
    "                }\n",
    "                index = 1\n",
    "                for detail_info_key in detail_info:\n",
    "                    detail = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/div[4]/ul/li[%d]/span[1]/a/text()'%index)\n",
    "                    index += 1\n",
    "                    if len(detail) != 0:\n",
    "                        detail_info[detail_info_key] = detail[0]\n",
    "                # print(detail_info)\n",
    "\n",
    "                # 激动人心的时刻！最重要的东西是做菜的步骤啊！\n",
    "                method_details = ''\n",
    "                method_num = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/div[6]/ul/li/div[2]/div')\n",
    "                method = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/div[6]/ul/li/div[2]/text()')\n",
    "                for i in range(0,len(method_num)):\n",
    "                    method_details += method_num[i].text + '、'  + method[i] + '  '\n",
    "                # print(method_details)\n",
    "                data_lst.append((cuisine_name,main_ingredient_dict,secondary_ingredient_dict,spices_dict,detail_info,method_details))\n",
    "\n",
    "# 写到这忽然感觉我不如直接写.py文件，反正似乎也不怎么用分块的调试来着，接下来根据统一的格式设计存入 csv文件 的部分\n",
    "file_path = os.path.join(r'D:\\PYZ\\PT(Shi_xi_shi_xun)\\Group\\menu_tasks\\data', 'zhe')\n",
    "if not os.path.exists(file_path):\n",
    "    os.mkdir(file_path)\n",
    "\n",
    "with open(file_path + os.sep + 'dish.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['菜品名','口味','工艺','耗时','难度','步骤'])\n",
    "\n",
    "with open(file_path + os.sep + 'main_ingredient.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['名字'])\n",
    "\n",
    "with open(file_path + os.sep + 'secondary_ingredient.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['名字'])\n",
    "\n",
    "with open(file_path + os.sep + 'spices.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['名字'])\n",
    "\n",
    "with open(file_path + os.sep + 'ingredient_amount.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['原料名','用量','菜品名'])\n",
    "\n",
    "with open(file_path + os.sep + 'cuisine_dish.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['菜品名','属于','菜系'])\n",
    "\n",
    "# 避免重复存储\n",
    "main_ingredient_have_been_added = []\n",
    "secondary_ingredient_have_been_added = []\n",
    "spices_dict_have_been_added = []\n",
    "\n",
    "for data in data_lst:\n",
    "    # dish.csv\n",
    "    with open(file_path + os.sep + 'dish.csv', 'a', newline='',encoding='utf-8') as f: \n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([data[0],data[4]['口味'],data[4]['工艺'],data[4]['耗时'],data[4]['难度'],data[5]])\n",
    "    # main_ingredient.csv\n",
    "    with open(file_path + os.sep + 'main_ingredient.csv','a',newline='',encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for main_ingredient_name in data[1]:\n",
    "                if main_ingredient_name in main_ingredient_have_been_added:\n",
    "                    continue\n",
    "                main_ingredient_have_been_added.append(main_ingredient_name)\n",
    "                writer.writerow([main_ingredient_name])\n",
    "    # secondary_ingredient.csv            \n",
    "    with open(file_path + os.sep + 'secondary_ingredient.csv','a',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for secondary_ingredient_name in data[2]:\n",
    "            if secondary_ingredient_name in secondary_ingredient_have_been_added:\n",
    "                continue\n",
    "            secondary_ingredient_have_been_added.append(secondary_ingredient_name)\n",
    "            writer.writerow([secondary_ingredient_name])\n",
    "            \n",
    "                \n",
    "    with open(file_path + os.sep + 'spices.csv','a',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for spices_name in data[3]:\n",
    "            if spices_name in spices_dict_have_been_added:\n",
    "                continue\n",
    "            spices_dict_have_been_added.append(spices_name)\n",
    "            writer.writerow([spices_name])\n",
    "                \n",
    "\n",
    "    with open(file_path + os.sep + 'ingredient_amount.csv','a',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for main_ingredient_name in data[1]:\n",
    "            writer.writerow([main_ingredient_name,data[1][main_ingredient_name],data[0]])\n",
    "        for secondary_ingredient_name in data[2]:\n",
    "            writer.writerow([secondary_ingredient_name,data[2][secondary_ingredient_name],data[0]])\n",
    "        for spices_name in data[3]:\n",
    "            writer.writerow([spices_name,data[3][spices_name],data[0]])\n",
    "\n",
    "    with open(file_path + os.sep + 'cuisine_dish.csv','a',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([data[0],'属于','浙菜'])\n",
    "print('END...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 闽菜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END...\n"
     ]
    }
   ],
   "source": [
    "# 生成 urls list\n",
    "urls_mincai = ['https://home.meishichina.com/recipe/mincai/page/{}/'.format(str(i)) for i in range(1, 4)]\n",
    "data_lst = []\n",
    "# 遍历每一个列表页\n",
    "for url_mincai in urls_mincai:\n",
    "    # 封装发送请求并获取响应\n",
    "    response = requests.get(url=url_mincai, headers=request_headers)\n",
    "    # 判断状态码\n",
    "    status_code = response.status_code\n",
    "    if status_code == 200:\n",
    "        html_text = response.text\n",
    "        # 解析 html 文本\n",
    "        bs = BeautifulSoup(html_text, \"lxml\")\n",
    "        # 定位到详情页链接的 a 标签\n",
    "        urls_cuisine = bs.select('#J_list > ul > li > div.detail > h2 > a')\n",
    "        img_infos = bs.select('#J_list > ul > li > div.pic > a > img')\n",
    "        # 遍历所有菜品\n",
    "        for cuisine, img_info in zip(urls_cuisine, img_infos):\n",
    "            # 提取详情页链接，即 href 属性值\n",
    "            url_cuisine = cuisine.get('href')\n",
    "            # 由于尝试发现详情页中图片不合适，所以回到列表页抓取图片\n",
    "            cuisine_name = cuisine.get('title')\n",
    "            pic_url = img_info.get('data-src')\n",
    "            resp = requests.get(pic_url, headers=request_headers)\n",
    "            # 检查响应状态码\n",
    "            if resp.status_code == 200:\n",
    "                # 获取图片内容\n",
    "                image_content = resp.content\n",
    "                file_name = cuisine_name + \".jpg\"\n",
    "                file_path = os.path.join(r'D:\\PYZ\\PT(Shi_xi_shi_xun)\\Group\\menu_tasks\\pictures', 'min')\n",
    "                if not os.path.exists(file_path):\n",
    "                    os.mkdir(file_path)\n",
    "\n",
    "                # 完成写入\n",
    "                with open(file_path + os.sep + file_name, 'wb') as fp:\n",
    "                    fp.write(image_content)\n",
    "\n",
    "            # 获取每一个菜品详情页面中的信息\n",
    "            # 封装发送请求并获取响应\n",
    "            response = requests.get(url=url_cuisine, headers=request_headers)\n",
    "            # 判断状态码\n",
    "            status_code = response.status_code\n",
    "            if status_code == 200:\n",
    "                # 防止中文乱码，因此自动识别编码方式后再提取文本\n",
    "                response.encoding = response.apparent_encoding\n",
    "                html_text = response.text\n",
    "                # 解析 html 文本\n",
    "                bs = BeautifulSoup(html_text, \"lxml\")\n",
    "                selector = etree.HTML(html_text)\n",
    "                # 获取菜品名称\n",
    "                cuisine_name = bs.select('body > div.wrap > div > div.space_left > div.userTop.clear > h1 > a')[0].get_text().strip()\n",
    "                # print(cuisine_name)\n",
    "                # 获取菜品主料\n",
    "                # 悲，本来觉得不难！结果从这第二个数据就卡死了难蚌！太难了！！！最难得是定位啊！！！有规律的网页是正规网页人家会反爬，哭；不反爬的小网页，它可能不规律，淦！\n",
    "                # 由于使用CSS选择器的定位方式，发现定位不好搞，因为fieldset后面的数字不同，也就是层数不一样（悲），但是！经过桐桐的提醒，发现xpath是有规律的！所以站在巨人的肩膀上干活效率会变高（bushi）\n",
    "                main_ingredient_dict = {}\n",
    "                main_ingredient_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[1]/div/ul/li/span[1]/a/b')\n",
    "                for i in range(0,len(main_ingredient_names)):\n",
    "                    main_ingredient_name = main_ingredient_names[i].text\n",
    "                    main_ingredient_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[1]/div/ul/li/span[2]')\n",
    "                    main_ingredient_dict[main_ingredient_name] = main_ingredient_dosage[i].text\n",
    "                # 这里是处理没有链接的那种情况，两种定位不一样\n",
    "                main_ingredient_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[1]/div/ul/li/span[1]/b')\n",
    "                if len(main_ingredient_names) != 0:\n",
    "                    for i in range(0,len(main_ingredient_names)):\n",
    "                        main_ingredient_name = main_ingredient_names[i].text\n",
    "                        main_ingredient_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[1]/div/ul/li/span[2]')\n",
    "                        main_ingredient_dict[main_ingredient_name] = main_ingredient_dosage[i].text\n",
    "                # print(main_ingredient_dict)\n",
    "\n",
    "                # 获取菜品辅料\n",
    "                # 为了避免麻烦，继续采用上面这个方法\n",
    "                secondary_ingredient_dict = {}\n",
    "                secondary_ingredient_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[2]/div/ul/li/span[1]/a/b')\n",
    "                for i in range(0,len(secondary_ingredient_names)):\n",
    "                    secondary_ingredient_name = secondary_ingredient_names[i].text\n",
    "                    secondary_ingredient_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[2]/div/ul/li/span[2]')\n",
    "                    secondary_ingredient_dict[secondary_ingredient_name] = secondary_ingredient_dosage[i].text\n",
    "                # 这里是处理没有链接的那种情况，两种定位不一样\n",
    "                secondary_ingredient_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[2]/div/ul/li/span[1]/b')\n",
    "                if len(secondary_ingredient_names) != 0:\n",
    "                    for i in range(0,len(secondary_ingredient_names)):\n",
    "                        secondary_ingredient_name = secondary_ingredient_names[i].text\n",
    "                        secondary_ingredient_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[2]/div/ul/li/span[2]')\n",
    "                        secondary_ingredient_dict[secondary_ingredient_name] = secondary_ingredient_dosage[i].text\n",
    "                # print(secondary_ingredient_dict)\n",
    "\n",
    "                # 获取菜品调料\n",
    "                # 突出一个追求效率（潜台词：懒）\n",
    "                spices_dict = {}\n",
    "                spices_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[3]/div/ul/li/span[1]/a/b')\n",
    "                for i in range(0,len(spices_names)):\n",
    "                    spices_name = spices_names[i].text\n",
    "                    spices_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[3]/div/ul/li/span[2]')\n",
    "                    spices_dict[spices_name] = spices_dosage[i].text\n",
    "                # 这里是处理没有链接的那种情况，两种定位不一样\n",
    "                spices_names = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[3]/div/ul/li/span[1]/b')\n",
    "                if len(spices_names) != 0:\n",
    "                    for i in range(0,len(spices_names)):\n",
    "                        spices_name = spices_names[i].text\n",
    "                        spices_dosage = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[3]/div/ul/li/span[2]')\n",
    "                        spices_dict[spices_name] = spices_dosage[i].text\n",
    "                # print(spices_dict)\n",
    "\n",
    "                # 获取食材明细最后一块 —— 其实应该是特性\n",
    "                # 口味 工艺 耗时 难度\n",
    "                detail_info = {\n",
    "                    '口味': None,\n",
    "                    '工艺': None,\n",
    "                    '耗时': None,\n",
    "                    '难度': None\n",
    "                }\n",
    "                index = 1\n",
    "                for detail_info_key in detail_info:\n",
    "                    detail = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/div[4]/ul/li[%d]/span[1]/a/text()'%index)\n",
    "                    index += 1\n",
    "                    if len(detail) != 0:\n",
    "                        detail_info[detail_info_key] = detail[0]\n",
    "                # print(detail_info)\n",
    "\n",
    "                # 激动人心的时刻！最重要的东西是做菜的步骤啊！\n",
    "                method_details = ''\n",
    "                method_num = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/div[6]/ul/li/div[2]/div')\n",
    "                method = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/div[6]/ul/li/div[2]/text()')\n",
    "                for i in range(0,len(method_num)):\n",
    "                    method_details += method_num[i].text + '、' + method[i] + '  '\n",
    "                # print(method_details)\n",
    "                data_lst.append((cuisine_name,main_ingredient_dict,secondary_ingredient_dict,spices_dict,detail_info,method_details))\n",
    "\n",
    "# 写到这忽然感觉我不如直接写.py文件，反正似乎也不怎么用分块的调试来着，接下来根据统一的格式设计存入 csv文件 的部分\n",
    "file_path = os.path.join(r'D:\\PYZ\\PT(Shi_xi_shi_xun)\\Group\\menu_tasks\\data', 'min')\n",
    "if not os.path.exists(file_path):\n",
    "    os.mkdir(file_path)\n",
    "\n",
    "with open(file_path + os.sep + 'dish.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['菜品名','口味','工艺','耗时','难度','步骤'])\n",
    "\n",
    "with open(file_path + os.sep + 'main_ingredient.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['名字'])\n",
    "\n",
    "with open(file_path + os.sep + 'secondary_ingredient.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['名字'])\n",
    "\n",
    "with open(file_path + os.sep + 'spices.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['名字'])\n",
    "\n",
    "with open(file_path + os.sep + 'ingredient_amount.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['原料名','用量','菜品名'])\n",
    "\n",
    "with open(file_path + os.sep + 'cuisine_dish.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['菜品名','属于','菜系'])\n",
    "\n",
    "# 避免重复存储\n",
    "main_ingredient_have_been_added = []\n",
    "secondary_ingredient_have_been_added = []\n",
    "spices_dict_have_been_added = []\n",
    "\n",
    "for data in data_lst:\n",
    "    # dish.csv\n",
    "    with open(file_path + os.sep + 'dish.csv', 'a', newline='',encoding='utf-8') as f: \n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([data[0],data[4]['口味'],data[4]['工艺'],data[4]['耗时'],data[4]['难度'],data[5]])\n",
    "    # main_ingredient.csv\n",
    "    with open(file_path + os.sep + 'main_ingredient.csv','a',newline='',encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for main_ingredient_name in data[1]:\n",
    "                if main_ingredient_name in main_ingredient_have_been_added:\n",
    "                    continue\n",
    "                main_ingredient_have_been_added.append(main_ingredient_name)\n",
    "                writer.writerow([main_ingredient_name])\n",
    "    # secondary_ingredient.csv            \n",
    "    with open(file_path + os.sep + 'secondary_ingredient.csv','a',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for secondary_ingredient_name in data[2]:\n",
    "            if secondary_ingredient_name in secondary_ingredient_have_been_added:\n",
    "                continue\n",
    "            secondary_ingredient_have_been_added.append(secondary_ingredient_name)\n",
    "            writer.writerow([secondary_ingredient_name])\n",
    "            \n",
    "                \n",
    "    with open(file_path + os.sep + 'spices.csv','a',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for spices_name in data[3]:\n",
    "            if spices_name in spices_dict_have_been_added:\n",
    "                continue\n",
    "            spices_dict_have_been_added.append(spices_name)\n",
    "            writer.writerow([spices_name])\n",
    "                \n",
    "\n",
    "    with open(file_path + os.sep + 'ingredient_amount.csv','a',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for main_ingredient_name in data[1]:\n",
    "            writer.writerow([main_ingredient_name,data[1][main_ingredient_name],data[0]])\n",
    "        for secondary_ingredient_name in data[2]:\n",
    "            writer.writerow([secondary_ingredient_name,data[2][secondary_ingredient_name],data[0]])\n",
    "        for spices_name in data[3]:\n",
    "            writer.writerow([spices_name,data[3][spices_name],data[0]])\n",
    "\n",
    "    with open(file_path + os.sep + 'cuisine_dish.csv','a',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([data[0],'属于','闽菜'])\n",
    "print('END...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_7_virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
