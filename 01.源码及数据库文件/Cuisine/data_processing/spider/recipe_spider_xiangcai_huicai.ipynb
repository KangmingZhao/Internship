{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3902a99c-721c-4765-9a4a-2e435de65a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import lxml\n",
    "import bs4\n",
    "import requests\n",
    "BeautifulSoup = bs4.BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "#上面是基本上没什么卵用了已经的静态页面爬虫\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import ActionChains\n",
    "#这里是自动化操作浏览器的工具\n",
    "import cv2\n",
    "import pytesseract\n",
    "#这里是处理截图的工具\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a064c-a842-4c4e-b77d-c19f2cb85cdc",
   "metadata": {},
   "source": [
    "我负责的部分是湘菜和徽菜。我主要是想使用selenium来进行。我不打算使用记录每一个页面的规律，而是采用模拟点击翻页的方法来进行。也是要挨个儿点进每个菜谱的具体页面爬取。关于截图然后opencv，这个看情况看用不用的上。毕竟opencv的截图方法姑且还是基于概率的，即使正确率再高，比起直接从页面拿到数据，还是有错误的可能的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a9094f-d4be-48f3-b678-ab5684c06304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#姑且还是把请求头拿过来。\n",
    "user_agents = [\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n",
    "    \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n",
    "]\n",
    "#随机拿一个\n",
    "request_headers = {\n",
    "    'user-agent':\n",
    "        random.choice(user_agents),\n",
    "    'Connection':\n",
    "        \"keep-alive\",\n",
    "    'Referer':\n",
    "        \"https://www.douban.com\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad8c660-fae8-406b-bd44-5deee9556bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipt_list_url = {\n",
    "    '湘菜':'https://home.meishichina.com/recipe/xiangcai/',\n",
    "    '徽菜':'https://home.meishichina.com/recipe/huicai/'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18decf-3781-47fc-add7-9c375c9a67e5",
   "metadata": {},
   "source": [
    "为了不让数据量beyond what we can manage,我们打算每个菜系爬30道，也就是三页。\n",
    "\n",
    "模拟人类操作，统一在所有点击操作之前进行sleep。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255b3e20-6c62-4601-95a9-45d12813d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Edge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b255de7f-8960-4a02-8c9e-9379a84ad5cb",
   "metadata": {},
   "source": [
    "现在先开始分析xpath。\n",
    "\n",
    "第一个菜的连接的xpath是：//*[@id=\"J_list\"]/ul/li[1]/div[2]/h2/a\n",
    "\n",
    "第一个菜的连接的xpath是：//*[@id=\"J_list\"]/ul/li[2]/div[2]/h2/a\n",
    "\n",
    "显然由于一页有10个详情页，所以['//*[@id=\"J_list\"]/ul/li[%d]/div[2]/h2/a'%i for i in range(1,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ef9137-8631-4ae3-973d-acf48edafae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait(second):\n",
    "    time.sleep(random.uniform(0.7,0.7 + second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb04072-43f0-412f-b327-f4d68367fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_2_text(driver,target_element,img_path):\n",
    "    # 使用 ActionChains 将鼠标悬停在目标元素上，以确保其完全可见\n",
    "    ActionChains(driver).move_to_element(target_element).perform()\n",
    "    target_element.screenshot(img_path)\n",
    "\n",
    "    \n",
    "    image = cv2.imread(img_path)\n",
    "    \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    text = pytesseract.image_to_string(image_rgb, lang=\"chi_sim\")\n",
    "\n",
    "#     gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     _, binary_image = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "#     recognized_text = pytesseract.image_to_string(gray_image, lang='chi_sim')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da129bd4-efd4-4903-9358-98cd06ca2dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_receipt_info(driver, now_url):\n",
    "    resp = requests.get(url = now_url,headers = request_headers)\n",
    "    status_code = resp.status_code\n",
    "    if status_code == 200:\n",
    "        resp.encoding = 'utf-8'\n",
    "        html_text = resp.text\n",
    "        soup_document = BeautifulSoup(html_text, 'lxml')\n",
    "        selector = lxml.etree.HTML(html_text)\n",
    "        \n",
    "        \n",
    "        #-----------------------------即使没有必要，我这里也打算用多种不同的方式来拿到数据---------------------\n",
    "        \n",
    "        #菜品名字使用css选择器来找\n",
    "        #这是菜品名字#recipe_title\n",
    "        dish_name = soup_document.select('#recipe_title')[0].get_text().strip()\n",
    "        \n",
    "\n",
    "        \n",
    "        #然后是主料辅料和调料\n",
    "        #首先是主料\n",
    "        #ingredients_loader = soup_document.select('body > div.wrap > div > div.space_left > div.space_box_home > div > fieldset[1] > div > ul > li > span.category_s1 > a > b')\n",
    "        #主料必须用xpath而不是css。因为不同页面主料对应的fieldset是不一样的。有的是7.有的是8.但是xpath这里都是从1到3，有规律可循\n",
    "        main_ingredients_loader = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[1]/div/ul/li/span[1]/a/b')\n",
    "        main_ingredient_dict = {}\n",
    "        for i in range(0,len(main_ingredients_loader)):\n",
    "            wait(1)\n",
    "            main_ingredient_name = main_ingredients_loader[i].text\n",
    "            \n",
    "            picture_xpath = '/html/body/div[5]/div/div[1]/div[3]/div/fieldset[1]/div/ul/li[%d]/span[2]'%(i+1)\n",
    "            #好吧，看来还是用上了opencv。这里不知道为什么用量截取不下来，那么我只好截图了（喜）\n",
    "            #首先给具体数量截个图\n",
    "            #啊啊啊啊啊气死我了适量识别不出来,好在适量可以手动添加\n",
    "            amount_picture_element = driver.find_element(By.XPATH, picture_xpath)\n",
    "            amount_picture_text = img_2_text(driver,amount_picture_element,'main_ingredient_amount.png').replace('\\n','')\n",
    "            main_ingredient_dict[main_ingredient_name] = amount_picture_text if amount_picture_text else '适量'\n",
    "        #print(main_ingredient_dict)\n",
    "                                                    \n",
    "        secondary_ingredients_loader = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[2]/div/ul/li/span[1]/a/b')\n",
    "        secondary_ingredient_dict = {}\n",
    "        for i in range(0,len(secondary_ingredients_loader)):\n",
    "            wait(1)\n",
    "            secondary_ingredient_name = secondary_ingredients_loader[i].text\n",
    "            \n",
    "            picture_xpath = '/html/body/div[5]/div/div[1]/div[3]/div/fieldset[2]/div/ul/li[%d]/span[2]'%(i+1)\n",
    "            amount_picture_element = driver.find_element(By.XPATH, picture_xpath)\n",
    "            amount_picture_text = img_2_text(driver,amount_picture_element,'secondary_ingredient_amount.png').replace('\\n','')\n",
    "            secondary_ingredient_dict[secondary_ingredient_name] = amount_picture_text if amount_picture_text else '适量'\n",
    "        #print(secondary_ingredient_dict)\n",
    "        \n",
    "        \n",
    "        spices_loader = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/fieldset[3]/div/ul/li/span[1]/a/b')\n",
    "        spices_dict = {}\n",
    "        for i in range(0,len(spices_loader)):\n",
    "            wait(1)\n",
    "            spices_name = spices_loader[i].text\n",
    "            \n",
    "            picture_xpath = '/html/body/div[5]/div/div[1]/div[3]/div/fieldset[3]/div/ul/li[%d]/span[2]'%(i+1)\n",
    "            amount_picture_element = driver.find_element(By.XPATH, picture_xpath)\n",
    "            amount_picture_text = img_2_text(driver,amount_picture_element,'spices__amount.png').replace('\\n','')\n",
    "            spices_dict[spices_name] = amount_picture_text if amount_picture_text else '适量'\n",
    "        #print(spices_dict)\n",
    "        \n",
    "        \n",
    "        #接下来是特点。这个东西好就好在非常固定，一共只有：\n",
    "        #口味 工艺 耗时 难度\n",
    "        detail_info = {\n",
    "            '口味':None,\n",
    "            '工艺':None,\n",
    "            '耗时':None,\n",
    "            '难度':None\n",
    "        }\n",
    "        #这是口味/html/body/div[5]/div/div[1]/div[3]/div/div[4]/ul/li[1]/span[1]/a\n",
    "        #这是工艺/html/body/div[5]/div/div[1]/div[3]/div/div[4]/ul/li[2]/span[1]/a\n",
    "        #可以看到还是有规律的\n",
    "        detail_info_index = 1\n",
    "        for i in detail_info:\n",
    "            wait(1)\n",
    "            detail_text = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/div[4]/ul/li[%d]/span[1]/a'%detail_info_index)[0].text\n",
    "            detail_info[i] = detail_text\n",
    "            detail_info_index += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        wait(1)\n",
    "        #最后是描述文本了。\n",
    "        #这个就是我们想要的东西。/html/body/div[5]/div/div[1]/div[3]/div/div[6]/ul/li/div[2]/text()\n",
    "        cooking_tutorial_text = ''\n",
    "        now_step = 1\n",
    "        cooking_tutorial_steps = selector.xpath('/html/body/div[5]/div/div[1]/div[3]/div/div[6]/ul/li/div[2]/text()')\n",
    "        for cooking_tutorial_step in cooking_tutorial_steps:\n",
    "            now_step_string = '%d、'%now_step\n",
    "            cooking_tutorial_text += now_step_string\n",
    "            cooking_tutorial_text += cooking_tutorial_step\n",
    "            cooking_tutorial_text += '  '\n",
    "            now_step += 1\n",
    "    \n",
    "        #好的现在我们已经拿到了所有需要的东西了。\n",
    "        data_return = (dish_name,main_ingredient_dict,secondary_ingredient_dict,spices_dict,detail_info,cooking_tutorial_text)\n",
    "        return data_return\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('fuck')\n",
    "        return None\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "791dad79-8e08-4801-ab2f-aef052a7603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(data_list, cuisine_type, a,b,c):\n",
    "    #先研究一下按什么格式写入csv。\n",
    "\n",
    "        \n",
    "    #需要定义一个ingredients_have_been_added,防止重复装入相同的原料\n",
    "    main_ingredient_have_been_added = a\n",
    "    secondary_ingredient_have_been_added = b\n",
    "    spices_dict_have_been_added = c\n",
    "\n",
    "    for i in data_list:\n",
    "        with open('dish.csv', 'a', newline='',encoding='utf-8') as f: \n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([i[0],i[4]['口味'],i[4]['工艺'],i[4]['耗时'],i[4]['难度'],i[5]])\n",
    "            \n",
    "            \n",
    "        with open('main_ingredient.csv','a',newline='',encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for main_ingredient_name in i[1]:\n",
    "                if main_ingredient_name in main_ingredient_have_been_added:\n",
    "                    continue\n",
    "                main_ingredient_have_been_added.append(main_ingredient_name)\n",
    "                writer.writerow([main_ingredient_name])\n",
    "\n",
    "    \n",
    "        with open('secondary_ingredient.csv','a',newline='',encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for secondary_ingredient_name in i[2]:\n",
    "                if secondary_ingredient_name in secondary_ingredient_have_been_added:\n",
    "                    continue\n",
    "                secondary_ingredient_have_been_added.append(secondary_ingredient_name)\n",
    "                writer.writerow([secondary_ingredient_name])\n",
    "            \n",
    "                \n",
    "        with open('spices.csv','a',newline='',encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for spices_name in i[3]:\n",
    "                if spices_name in spices_dict_have_been_added:\n",
    "                    continue\n",
    "                spices_dict_have_been_added.append(spices_name)\n",
    "                writer.writerow([spices_name])\n",
    "                \n",
    "\n",
    "        with open('ingredient_amount.csv','a',newline='',encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for main_ingredient_name in i[1]:\n",
    "                writer.writerow([main_ingredient_name,i[1][main_ingredient_name],i[0]])\n",
    "            for secondary_ingredient_name in i[2]:\n",
    "                writer.writerow([secondary_ingredient_name,i[2][secondary_ingredient_name],i[0]])\n",
    "            for spices_name in i[3]:\n",
    "                writer.writerow([spices_name,i[3][spices_name],i[0]])\n",
    "\n",
    "        with open('cuisine_dish.csv','a',newline='',encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([i[0],'属于',cuisine_type])\n",
    "            \n",
    "    return (main_ingredient_have_been_added,secondary_ingredient_have_been_added,spices_dict_have_been_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8cb9a9-8c6e-4b82-b76f-9d4e9d2f986b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af44586-52b5-4f10-9c67-6eb6f226a8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74cb3994-c325-4821-af4a-ea80f0830d16",
   "metadata": {},
   "source": [
    "这里比较生草的地方是，这个热门按钮按了之后就不再可交互了。我之前在这个地方老是告诉我找不到元素。之前爬哔哩哔哩真的给孩子爬出心理阴影了，现在看见这个找不到元素的报错就真的想哭。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfed085f-c7bd-44f9-9300-f8842ea526c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_receipt(receipt_url):\n",
    "    all_data = []\n",
    "    driver.get(receipt_url)\n",
    "    #先点击热门。我打算找最热门的30个。\n",
    "    #热门按钮的xpath是：/html/body/div[5]/div/div[1]/div[1]/div/a[1]\n",
    "    try:\n",
    "        wait(1)\n",
    "        driver.find_element (By.XPATH, '/html/body/div[5]/div/div[1]/div[1]/div/a[1]').click()\n",
    "    except Exception:\n",
    "        print('已经是最热了')\n",
    "    for _ in range(3):\n",
    "        #翻页什么的在函数末尾在考虑\n",
    "        \n",
    "        #首先在这里拿到我们当前的列表页的记录。\n",
    "        current_window_handle = driver.current_window_handle\n",
    "        \n",
    "        #现在要把这10个详情页挨个儿点进去。\n",
    "        content_pages = ['//*[@id=\"J_list\"]/ul/li[%d]/div[2]/h2/a'%i for i in range(1,11)]\n",
    "        picture_index = 1\n",
    "        for content_page in content_pages:\n",
    "            wait(1)\n",
    "            driver.find_element (By.XPATH, content_page).click()\n",
    "            #把driver切换到新的页面\n",
    "            all_window_handles = driver.window_handles\n",
    "            for window_handle in all_window_handles:\n",
    "                if window_handle != current_window_handle:\n",
    "                    wait(2)\n",
    "                    driver.switch_to.window(window_handle)\n",
    "                    break\n",
    "            \n",
    "            \n",
    "            \n",
    "            #上面的部分是实现翻页点击页面相关\n",
    "            #--------------------------------------------------------------------------------------\n",
    "            #这里是对页面进行具体爬取。我比较喜欢先验证了这个自动翻页进入页面的可行性。因为之前\n",
    "            #爬蜀黍的b站真的给孩子爬ptsd了。所以真的讨厌太正规的网站、、那么羞涩干什么怎么就不让我\n",
    "            #好好爬呢（恼），，，作为用户的时候责怪b站怎么不把自己搞得安全一点三天两头被黑，需要爬\n",
    "            #它的时候就想着这玩意怎么这么固若金汤、、这就是人性吗（悲）\n",
    "            now_url = driver.current_url\n",
    "            this_data = get_specific_receipt_info(driver, now_url)\n",
    "            all_data.append(this_data)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #--------------------------------------------------------------------------------------\n",
    "            #下面的部分是实现翻页点击页面相关\n",
    "            \n",
    "            #结束之后把当前页面关掉\n",
    "            wait(1)\n",
    "            driver.close()\n",
    "            #切换回列表页\n",
    "            wait(1)\n",
    "            driver.switch_to.window(current_window_handle)\n",
    "            \n",
    "            \n",
    "            dish_name = this_data[0]\n",
    "            #菜品图片在列表页扒。因为详情页的不太合适\n",
    "            #切换回来之后就可以截图了。因为现在拿到了菜品的名字\n",
    "            #这是图片的xpath： //*[@id=\"J_list\"]/ul/li[1]/div[1]/a/img\n",
    "            dish_picture_xpath = '//*[@id=\"J_list\"]/ul/li[%d]/div[1]/a/img'%picture_index\n",
    "            picture_index += 1\n",
    "            dish_picture_element = driver.find_element(By.XPATH, dish_picture_xpath)\n",
    "            # 使用 ActionChains 将鼠标悬停在目标元素上，以确保其完全可见\n",
    "            ActionChains(driver).move_to_element(dish_picture_element).perform()\n",
    "            dish_picture_element.screenshot('%s.jpg'%dish_name)\n",
    "            \n",
    "            #break\n",
    "        \n",
    "        #这里是翻页\n",
    "        #翻页的xpath：/html/body/div[5]/div/div[1]/div[3]/div/a[6]\n",
    "        wait(1)\n",
    "        driver.find_element (By.XPATH, '/html/body/div[5]/div/div[1]/div[3]/div/a[6]').click()\n",
    "        #break\n",
    "        \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de7d39a0-ab3c-47c1-9173-3f03e4bfe0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ft183\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:343: UserWarning: name used for saved screenshot does not match file type. It should end with a `.png` extension\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "caipu = []\n",
    "for i in receipt_list_url:\n",
    "    this_url = receipt_list_url[i]\n",
    "    caipu.append(get_receipt(this_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75d033c8-cb3f-4135-9177-5c2bcbc99fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('【湘菜】啤酒鸭——夏季里降火的大肉菜',\n",
       "  {'鸭': '半只', '啤酒': '600ml'},\n",
       "  {'青红椒': '适量', '葱': '适量', '姜': '适量', '蒜': '适量'},\n",
       "  {'生抽': '适量', '老抽': '适量', '糖': '适量', '盐': '适量', '八角': '适量'},\n",
       "  {'口味': '微辣', '工艺': '炖', '耗时': '一小时', '难度': '普通'},\n",
       "  '1、准备材料。  2、鸭洗净切块，辣椒切块。  3、炒锅热油，放干辣椒、姜、蒜爆香，放鸭肉翻炒。  4、大火煸炒至鸭肉收干不出水（出水多可以倒掉），并且炒出部分鸭油。  5、加生抽、老抽、糖、一个八角炒至上色。  6、加入一瓶啤酒。  7、大火烧开后，改中小火焖三十分钟。  8、待汤汁基本收干加青红椒、葱花翻炒，再加盐调味，即可起锅。  9、趁热吃吧。  ')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caipu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24dfd73b-7c56-4151-8886-9906d27107a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ingredient_have_been_added = []\n",
    "secondary_ingredient_have_been_added = []\n",
    "spices_dict_have_been_added = []\n",
    "\n",
    "    \n",
    "#首先是要把所有的菜品名写入。菜品名需要的下表是：0，4，5\n",
    "with open('dish.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['菜品名','口味','工艺','耗时','难度','步骤'])\n",
    "\n",
    "with open('main_ingredient.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['名字'])\n",
    "\n",
    "with open('secondary_ingredient.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['名字'])\n",
    "\n",
    "with open('spices.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['名字'])\n",
    "\n",
    "with open('ingredient_amount.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['原料名','用量','菜品名'])\n",
    "\n",
    "with open('cuisine_dish.csv','w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['菜品名','属于','菜系'])\n",
    "        \n",
    "for i,cusion_type in zip(caipu,receipt_list_url):\n",
    "    (main_ingredient_have_been_added,secondary_ingredient_have_been_added,spices_dict_have_been_added) = write_csv(i,cusion_type,main_ingredient_have_been_added,secondary_ingredient_have_been_added,spices_dict_have_been_added)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf2659-16d5-4ec2-b530-1adb0944a07e",
   "metadata": {},
   "source": [
    "在这个地方，我爬了1个小时。我不敢在这个ipynb的基础上改了。这个版本爬下来的图片有点问题，我将在另一个ipynb文件中爬取图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e70f8c-d384-4ffb-b113-d8255bf45bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
